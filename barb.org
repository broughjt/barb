#+title: Barb
#+date: <2023-12-25 Mon>
#+author: Jackson Brough

* Introduction

With minimal exceptions so far, I have written the proofs without
looking at anybody else's code first. I will try to explicitly state
where this isn't the case. For example, in Tao's Analyis 1 book, I
will look at the theorem statement and then try to prove it myself
before looking at the proof below. Obviously once I get past basic
arithmetic this ideal won't last very long -- I'll have to look at
other proofs to even get an idea of how to approach the
problem. However, my goal is to get as much insight from this project
as possible, so I'll have to find a working balance.

* Library
Right now, this project is a formalization of Terence Tao's Analyis I
[cite:@tao2022analysis]. My hope is to continue to other books, but
for now it pretty directly follows the structure of Tao's book.

While the library is small, I'm going to group things here into a
collection of utilities and then the files for the natural numbers,
integers, rationals, and reals, respectively. This won't correspond
directly to the actual file structure, but I think that's actually a
good thing. If I'm going to maintain some sort of literature component
to this library, I would like it to tell a story about my learning
path, and that will have a very different structure than the code
dependencies between modules.

#+begin_src lean4 :tangle Barb_org.lean
-- import «Barb».Algebra
import «Barb».Function
import «Barb».Logic
-- import «Barb».Order
-- import «Barb».Quotient
-- import «Barb».Syntax
--
import «Barb».Data.Natural
-- import «Barb».Data.Integer
-- import «Barb».Data.Rational

-- import «Barb».Data.Option
#+end_src

** Natural
:PROPERTIES:
:header-args: :tangle Barb/Data/Natural_org.lean
:END:

*** Introduction
"The positive integers and their arithmetic are presupposed by the
very nature of our intelligence and, we are tempted to believe, by the
very nature of intelligence in general. The development of the
positive integers from the primitive concept of the unit, the concept
of adjoining a unit, and the process of mathematical induction carries
complete conviction. In the words of Kronecker, the positive integers
were created by God."

"Kronecker forgot to mention how shoddy the work of man can be."

TODO: Cite

We're going to define the natural numbers and develop their basic
algebraic and order-theoretic properties.

#+begin_src lean4
import Barb.Function
import Barb.Logic
import Barb.Order
#+end_src

*** Definition and Peano axioms

The inductive definition of the natural numbers starts with an
assumption that we have a zero element in $\mathbb{N}$ -- and yes, the natural
numbers start at zero: people who think the natural numbers start at
one have no class. Then we assume that for any $n$ in $\mathbb{N}$,
the successor $S(n)$ of that number is also in $\mathbb{N}$. In type theory,
just like everything else, we can represent this structure with an
inductive type.

#+begin_src lean4
inductive Natural where
  | zero : Natural
  | successor : Natural → Natural
#+end_src

This inductive type declaration adds two constructors, ~zero~ and
~successor~, and an eliminator which is a principle of recursion,
and as a special case of this, a principle of induction. These three
correspond to axioms 2.1, 2.2, and 2.5
[cite:@tao2022analysis, p.16-18]. What's cool is that the remaining
Peano axioms are theorems in our construction. This falls out of the
interesting properties of constructors in type theory, which I don't
fully understand yet.

First, we define conversions between our ~Natural~ type and the built
in ~Nat~ to be able to write $3 : \mathbb{N}$ instead of
~successor(successor(successor(zero))) : Natural~.

#+begin_src lean4
namespace Natural

open Natural (zero successor)

def fromNat : Nat → Natural
  | Nat.zero => Natural.zero
  | Nat.succ n => Natural.successor (fromNat n)

def toNat : Natural → Nat
  | Natural.zero => Nat.zero
  | Natural.successor n => Nat.succ (toNat n)

instance : OfNat Natural n where
  ofNat := fromNat n

instance : ToString Natural where
  toString := toString ∘ toNat

notation "ℕ" => Natural
#+end_src

Tao includes two other Peano axioms. Axiom 2.3 states that zero is not the successor of any natural number,
and axiom 2.4 states that the successor operation is injective, that
is, if $S(n) = S(m)$ then $n = m$.

I went on a long journey trying to understand ~noConfusion~, where I
read [[https://xenaproject.wordpress.com/2018/03/24/no-confusion-over-no_confusion/]["No confusion over no_confusion"]] and attempted to write my own
version. I think at one point I slightly grasped it, but really, I am
still confused. I especially don't understand why the $id$ nonsense
works in ~successor_injective~ -- I just threw terms around until they
type checked.

#+begin_src lean4
theorem successor_not_equal_zero (n : ℕ) : successor n ≠ 0 :=
  Natural.noConfusion

theorem successor_injective : Function.Injective successor :=
  λ h => (Natural.noConfusion h) id
#+end_src

Now we prove that the successor of a number is never equal to that
number. The proof works by applying the injectivity of the successor
in the inductive step, forming a long chain of deductive steps stemming
from the fact that zero is not the successor of any element of $\mathbb{N}$.

*** Addition

The proofs that follow were my first real exposure to proving things
in Lean. I had worked with Coq tactic proofs before, but I wanted to
understand term-style proofs in Lean instead of just playing Whac-A-Mole
with tactics I didn't understand yet. People talk about how Coq can
feel like a video game where you don't really understand what anything
means or what you're proving, but you just look at the current state
make moves that get you closer to the goal state -- this was
definitely my experience with most of the proofs I wrote in Coq.

I think starting with pure term-style proofs was very worth it. I
learned how inductive proofs match up with the recursor, and proofs of
equality or negation that were total magic to me in Coq make intuitive
sense to me now. After I had done a dozen or so of these, I read the
[[https://leanprover.github.io/theorem_proving_in_lean4/tactics.html][tactics chapter]] in Theorem Proving in Lean 4 and started revising the
proofs to use tactics.

What I'll do for the following theorems is provide some explanation,
then show my original term-style proof if it exists (cleaned up a bit
to match how I learned to do things later), and then finally show my
revised tactic-style proof which will be the one that actually gets
tangled. Even so, while writing the natural numbers module I was still
opposed to using much of the rewriter or the simplifier because I
didn't understand how they worked. You'll see me use those tactics
freely in the development of the integers and the rationals, but I
think it's kind of nice to make the proofs here as explicit as
possible.

TODO: Why? Is here a good place to explain the developmental nature of
the writing?

Addition is defined to be repeated application of the successor. To
add four to five is the same as incrementing five four times. We can
give a recursive definition as follows.

#+begin_src lean4
def add : ℕ → ℕ → ℕ
  | zero, m => m
  | successor n, m => successor (add n m)

instance : Add Natural where
  add := add

@[simp] theorem add_definition : add n m = n + m := rfl
#+end_src

I'm definitely a recurse-on-the-left kind of guy, but we could have
recursed on the right. Now in type theory this decision actually has
consequences due to definitional equality. The our case, the equations
$0 + n = n$ and $S(n) + m = S(n + m)$ hold definitionally, but 
$n + 0 = n$ and $n + S(m) = S(n + m)$ requires proofs. Luckily we
can prove these statements with induction, and then show that addition
is commutative, which will make our choice of definition less of a big
deal.

#+begin_src lean4
theorem zero_add (n : ℕ) : 0 + n = n := rfl

theorem successor_add (n m : ℕ) : (successor n) + m = successor (n + m) := rfl
#+end_src

The proof that $n + 0 = n$ works by rewriting $S(n) + 0$ to $S(n + 0)$
using the definition and then applying the inductive hypothesis which
claims that $n + 0 = n$ for an arbitary $n : \mathbb{N}$. This is
easier to see in the tactic-style proof.

#+begin_src lean4 :tangle no
theorem add_zero' (n : ℕ) : n + 0 = n := 
  Natural.rec
    (zero_add 0)
    (λ (x : ℕ) (ih : x + 0 = x) =>
      have h1 : (successor x) + 0 = successor (x + 0) := successor_add x 0
      have h2 : successor (x + 0) = (successor x) + 0 := Eq.symm h1
      have h3 : successor x = (successor x) + 0 := 
        Eq.subst (motive := λ a => successor a = (successor x) + 0)
          ih
          h2
      show (successor x) + 0 = successor x from Eq.symm h3)
    n
#+end_src

#+begin_src lean4
@[simp] theorem add_zero (n : ℕ) : n + 0 = n := by
  induction n with
  | zero => exact zero_add 0
  | successor n ih => calc
    (successor n) + 0 = successor (n + 0) := successor_add n 0
    _                 = successor n       := congrArg successor ih
#+end_src

In the term style proof, I didn't yet know about ~congrArg~ -- which
is wonderful -- and so I had to hack together equality substitutions,
which is what ~congrArg~ generalizes. I also used the ~have~ and
~show~ keywords, since they are just nice ways of writing ~let~ and
explicitly declaring the type of an expression, respectively, and this
really didn't feel like any big jump from pure lambda terms.

The tactic style proof is much cleaner. I really got into the
calc-style proof of lining up transitive relations step-wise.

The next proof is very similar to the last theorem: we do two
rewrites using the definition of addition and the inductive hypothesis
and we're done. I discovered ~congrArg~ while writing the term-style
proof for this theorem. It's awesome; it saves you from having to
prove the same throw-away mini lemmas over and over.

#+begin_src lean4 :tangle no
theorem add_successor' (n m : ℕ) : n + (successor m) = successor (n + m) :=
  Natural.rec
    (
      have h1 : 0 + (successor m) = successor m := zero_add (successor m)
      -- congrArg to the rescue!
      have h2 : successor (0 + m) = successor m := congrArg successor (zero_add m)
      show 0 + (successor m) = successor (0 + m) from Eq.trans h1 (Eq.symm h2)
    )
    (λ (x : ℕ) (ih : x + (successor m) = successor (x + m)) =>
      have h1 : (successor x) + (successor m) = successor (x + (successor m)) := successor_add x (successor m)
      have h2 : successor (x + (successor m)) = successor (successor (x + m)) := congrArg successor ih
      -- Little extra help from the compiler since (successor x) + m) is definitionally equal to sucessor (x + m)
      show (successor x) + (successor m) = successor ((successor x) + m) from Eq.trans h1 h2
    )
    n
#+end_src

#+begin_src lean4
theorem add_successor (n m : ℕ) : n + (successor m) = successor (n + m) := by
  induction n with
  | zero => calc
    0 + (successor m) = successor m       := zero_add (successor m)
    _                 = successor (0 + m) := congrArg successor (zero_add m)
  | successor x ih => calc
    (successor x) + (successor m) = successor (x + (successor m)) := successor_add x (successor m)
    _                             = successor (successor (x + m)) := congrArg successor ih
#+end_src

Now we develop the commutativity, associativity, and cancellation laws
for addition.

Why is addition commutative? Incrementing $x$ $y$ times always gives the same
result as incrementing $y$ $x$ times. This bears itself it out in the
proof: we make direct use of the two theorems we just proved. I had
term-style proofs of all of these, but I will spare you.

#+begin_src lean4
theorem add_commutative (n m : ℕ) : n + m = m + n := by
  induction n with
  | zero => calc
    0 + m = m     := zero_add m
    _     = m + 0 := (add_zero m).symm
  | successor x ih => calc
    (successor x) + m = successor (x + m) := successor_add x m
    _                 = successor (m + x) := congrArg successor ih
    _                 = m + (successor x) := (add_successor m x).symm
#+end_src

It's a bit harder to come with an intuitive explanation for
associativity. The order just doesn't matter man, that's all I have
for you. The proof's nice though, you just move the successor to the
front using the theorems we developed for right hand side successor
addition and then you rewrite inside the successor using the inductive
hypothesis.

#+begin_src lean4
theorem add_associative (n m k : ℕ) : (n + m) + k = n + (m + k) := by
  induction n with
  | zero => calc
    (0 + m) + k = m + k       := congrArg (. + k) (zero_add m)
    _           = 0 + (m + k) := zero_add (m + k)
  | successor x ih => calc
    ((successor x) + m) + k = (successor (x + m)) + k := congrArg (. + k) (successor_add x m)
    _                       = successor ((x + m) + k) := successor_add (x + m) k
    _                       = successor (x + (m + k)) := congrArg successor ih
#+end_src

Jackson from the future here, these theorems end up being useful when
you're rewriting heavily nested expressions, but they really should
just not exist; if only the simplifier was better.

#+begin_src lean4
theorem add_left_commutative (n m k : ℕ) : n + (m + k) = m + (n + k) := by
  rw [← add_associative, add_commutative n m, add_associative]
  
theorem add_right_commutative (n m k : ℕ) : (n + m) + k = (n + k) + m := by
  rw [add_associative, add_commutative m k, ← add_associative]
#+end_src

Addition is left cancellative because if $n + m$ and $n + k$ are the
same value then $m = k$ had better be the same value, that's why. The
real proof is inductive and utilizes the fact that zero is an
additive identity and that the successor is injective.

#+begin_src lean4
theorem add_left_cancel {n m k : ℕ} : n + m = n + k → m = k := by
  induction n with
  | zero => 
    intro h
    calc
      m = 0 + m := zero_add m
      _ = 0 + k := h
      _ = k     := zero_add k
  | successor x ih =>
    intro h
    have := calc
      successor (x + m) = (successor x) + m := (successor_add x m).symm
      _                 = (successor x) + k := h
      _                 = successor (x + k) := successor_add x k
    exact ih (successor_injective this)

theorem add_right_cancel {n m k : ℕ} (h : n + k = m + k) : n = m := by
  rw [add_commutative n k, add_commutative m k] at h
  exact add_left_cancel h
#+end_src



*** Decidable equality

A decision procedure for equality of natural numbers.

Once you give up the law of the excluded middle, you get this really
cool distinction between the decidability and non-decidability of
propositions. You can prove that a certain class of propositions /is/
decidable, that is, that either $p$ or $\neg p$ holds.

You accomplish this by giving a _decision procedure_ which shows how
to "decide" a predicate -- and mind you, a predicate is a function
which sends values to propositions, or a value-indexed family of
statements. It is *not* a function which returns a
boolean. Identifying statements with elements of $\{\top, \bot\}$ is
something from classical logic that feels obviously super messed up to
me!

So a decision procedure takes a predicate of the form ~p : α → Prop~
and an element of that type ~a : α~, and _decides_ ~p a~ by providing
a proof of ~p a~ or a proof ~¬(p a)~. Giving a decision procedure for
a predicate ~p~ shwos that ~p~ is _decidable_, because given any
instance ~a : α~, we have an algorithm for showing whether ~p a~ or
not ~p a~.

This section has gone through several different iterations, but the
latest version includes a small theory about the ~distance~ function
for natural numbers, which basically gives the absolute value
difference between the two values, except that we don't compute it
using subtraction, because we tried that earlier and natural number
subtraction is super broken total hack.

You'll notice that I'm totally willing to use the ~rw~ and ~simp~
tactics here. This might feel anachronistic, since I avoid them
everywhere else in the natural number proofs and that's because it is:
I came from the future because I need these theorems for the integers.

#+begin_src lean4
@[simp]
def distance : ℕ → ℕ → ℕ
  | zero, zero => 0
  | successor n, zero => successor n
  | zero, successor m => successor m
  | successor n, successor m => distance n m

theorem equal_of_distance_equal_zero : ∀ {n m : ℕ}, distance n m = 0 → n = m
  | zero, zero, _ => rfl
  | successor n, successor m, h => by
    unfold distance at h
    exact congrArg successor (equal_of_distance_equal_zero h)

theorem distance_equal_zero_of_equal : ∀ {n m : ℕ}, n = m → distance n m = 0
  | zero, zero, _ => rfl
  | successor n, successor m, h => by
    unfold distance
    exact distance_equal_zero_of_equal (successor_injective h)

theorem distance_self : ∀ (n : ℕ), distance n n = 0 :=
  λ _ => distance_equal_zero_of_equal rfl

theorem distance_zero_left : ∀ (n : ℕ), distance n 0 = n
  | zero => rfl
  | successor n => by unfold distance; rfl

theorem distance_commutative : ∀ (n m : ℕ), distance n m = distance m n
  | zero, zero => distance_zero_left _
  | zero, successor _ => distance_zero_left (successor _)
  | successor _, zero => distance_zero_left (successor _)
  | successor _, successor _ => by
    simp
    apply distance_commutative

theorem distance_zero_right (n : ℕ) : distance 0 n = n := by
  rw [distance_commutative, distance_zero_left]

theorem distance_add_add_right (n m k : ℕ) : distance (n + k) (m + k) = distance n m := by
  induction k with
  | zero => rw [← zero_definition, add_zero, add_zero]
  | successor k ih =>
    simp [add_successor]
    exact ih

theorem distance_add_add_left (n m k : ℕ) : distance (n + m) (n + k) = distance m k := by
  rw [add_commutative n m, add_commutative n k, distance_add_add_right]
#+end_src

I still can't figure out what to call this next theorem. It states
that if two sums are equal, the distance between the first terms of
each sum must be made up for exactly in the distance between the
second terms.

Writing this out visually is also compelling. Arrange two equal length lines
cut into two different length segments, name the segments $n$, $m$,
$k$, and $l$, and then arrange a copy of each segment vertically so
that their left ends line up. It will become apparent that the
difference in lengths between $n$ and $k$ match up with the difference
between $l$ and $m$.

#+begin_src lean4
theorem distance_equal_of_add_equal {n m k l : ℕ} (h : n + m = k + l) : distance n k = distance l m := by
  calc
    distance n k = distance (n + m) (k + m) := (distance_add_add_right n k m).symm
    _ = distance (k + l) (k + m) := congrArg (λ x => distance x _) h
    _ = distance l m := distance_add_add_left k l m
#+end_src

Since we have shown that $\mathnormal{distance}(n, m) = 0$ and $n = m$
are logically equivalent, we can use the distance function to decide
equality. This will be a common theme. When we develop decision
procedures for equality or less than or less equal relations, we often
establish a logical equivalence between something we know how to
compute and the proposition we want to decide. The connection between
decidable propositions, computability, and computational complexity
seems to be very important, and I'm interested to go deeper on this
idea.

#+begin_src lean4
instance decideEqual : DecidableEq Natural
  | n, m => match h : distance n m with
    | zero => isTrue (equal_of_distance_equal_zero h)
    | successor a => isFalse (mt distance_equal_zero_of_equal (h ▸ successor_not_equal_zero a))
#+end_src

Corollary 2.2.9 [cite:@tao2022analysis] utilizes proof by
contradiction. Corolarries are supposed to follow easily from a
previously stated theorem, but the proof of
~equal_zero_of_add_equal_zero~ was way, way more involved then
~add_positive~, which it was supposed to follow from. Now that we have
decidability, we can employ our classical proof methods
locally. Specifically, we use double negation here, which in this case
says that that $\neg \neg n = m \to n = m$.

#+begin_src lean4
theorem add_positive {n m : ℕ} : n ≠ 0 → (n + m) ≠ 0 :=
  match n with
  | zero => absurd rfl
  | successor x => λ _ => successor_not_equal_zero (x + m)

theorem equal_zero_of_add_equal_zero {n m : ℕ} (h : n + m = 0) : n = 0 ∧ m = 0 := by
  apply And.intro
  . exact Decidable.of_not_not (mt add_positive (not_not_intro h))
  . have : m + n = 0 := (add_commutative n m).symm.trans h
    exact Decidable.of_not_not (mt add_positive (not_not_intro this))

theorem unique_predecessor_of_positive {n : ℕ} : n ≠ 0 → ∃! (m : ℕ), successor m = n :=
  match n with
  | zero => absurd rfl
  | successor x => λ _ => ExistsUnique.introduction x rfl (λ _ => successor_injective)
#+end_src

*** Order, addition part
*** Multiplication
*** Order, multiplication part
*** Division algorithm and exponentiation

This just a stub for now, I would like to rewrite ~quotient_remainder~
into a type-level algorithm using subtypes and rename it to ~divideWithRemainder~.

** Integer
** Rational
** Utilities
*** Function
*** Logic
*** Order
*** Quotient
*** Syntax
